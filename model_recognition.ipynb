{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем все, что понадобится"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "import pytesseract\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random, pickle, glob\n",
    "from collections import defaultdict\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кучка необходимых параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "# тут можно поскладировать распознанные картинки\n",
    "dir_for_recognized_bounds = 'bounds_recognition_res'\n",
    "# тут будут складироваться картинки графа + обрезанных нод/ребер + распознанный текст + результат в виде списка смежности\n",
    "dir_for_recognized_graphs = 'graph_recognition_res'\n",
    "\n",
    "# тут должны лежать графы в приличном виде, на которых погоняем алгоритм\n",
    "validation_data_dir = 'graph_img_val'\n",
    "# тяжело, но хардкод до .exe \n",
    "path_to_tesseract_exe = os.path.join('D:\\Soft\\Anaconda\\Library', 'bin', 'tesseract.exe')\n",
    "pytesseract.pytesseract.tesseract_cmd = path_to_tesseract_exe\n",
    "\n",
    "# улучшалка для работы tesseract (распознавалка текстовых меток)\n",
    "custom_config = r'--oem 3 --psm 6 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "\n",
    "# Флаги, чтобы рисовать всякое ненужное, появляющееся в процессе\n",
    "draw_images_mode = False\n",
    "write_recognized_text = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Здесь читаем все .json в директории с данными = графы, которые можно будет распознавать.\n",
    "# Только для валидационных/тестовых данных\n",
    "def get_dict_list(dir_name: str):\n",
    "    file_list = glob.glob(os.path.join(dir_name, '*.json'))\n",
    "    \n",
    "    dict_list = []\n",
    "    for file in file_list:\n",
    "        with open(file) as f:\n",
    "            json_file = json.load(f)\n",
    "            \n",
    "            filename = json_file['filename'].split(\"\\\\\")[-1]\n",
    "            json_file['filename'] = os.path.join(dir_name, filename)\n",
    "            json_file['file_name'] = json_file.pop('filename')\n",
    "            \n",
    "            dict_list.append(json_file)\n",
    "            \n",
    "    return dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[12/23 16:31:55 d2.checkpoint.c2_model_loading]: \u001b[0mFollowing weights matched with model:\n",
      "| Names in Model                                  | Names in Checkpoint                                                                                  | Shapes                                          |\n",
      "|:------------------------------------------------|:-----------------------------------------------------------------------------------------------------|:------------------------------------------------|\n",
      "| backbone.bottom_up.res2.0.conv1.*               | backbone.bottom_up.res2.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,1,1)             |\n",
      "| backbone.bottom_up.res2.0.conv2.*               | backbone.bottom_up.res2.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |\n",
      "| backbone.bottom_up.res2.0.conv3.*               | backbone.bottom_up.res2.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |\n",
      "| backbone.bottom_up.res2.0.shortcut.*            | backbone.bottom_up.res2.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (256,) (256,) (256,) (256,) (256,64,1,1)        |\n",
      "| backbone.bottom_up.res2.1.conv1.*               | backbone.bottom_up.res2.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,256,1,1)            |\n",
      "| backbone.bottom_up.res2.1.conv2.*               | backbone.bottom_up.res2.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |\n",
      "| backbone.bottom_up.res2.1.conv3.*               | backbone.bottom_up.res2.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |\n",
      "| backbone.bottom_up.res2.2.conv1.*               | backbone.bottom_up.res2.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,256,1,1)            |\n",
      "| backbone.bottom_up.res2.2.conv2.*               | backbone.bottom_up.res2.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (64,) (64,) (64,) (64,) (64,64,3,3)             |\n",
      "| backbone.bottom_up.res2.2.conv3.*               | backbone.bottom_up.res2.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,64,1,1)        |\n",
      "| backbone.bottom_up.res3.0.conv1.*               | backbone.bottom_up.res3.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,256,1,1)       |\n",
      "| backbone.bottom_up.res3.0.conv2.*               | backbone.bottom_up.res3.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |\n",
      "| backbone.bottom_up.res3.0.conv3.*               | backbone.bottom_up.res3.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |\n",
      "| backbone.bottom_up.res3.0.shortcut.*            | backbone.bottom_up.res3.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (512,) (512,) (512,) (512,) (512,256,1,1)       |\n",
      "| backbone.bottom_up.res3.1.conv1.*               | backbone.bottom_up.res3.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |\n",
      "| backbone.bottom_up.res3.1.conv2.*               | backbone.bottom_up.res3.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |\n",
      "| backbone.bottom_up.res3.1.conv3.*               | backbone.bottom_up.res3.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |\n",
      "| backbone.bottom_up.res3.2.conv1.*               | backbone.bottom_up.res3.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |\n",
      "| backbone.bottom_up.res3.2.conv2.*               | backbone.bottom_up.res3.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |\n",
      "| backbone.bottom_up.res3.2.conv3.*               | backbone.bottom_up.res3.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |\n",
      "| backbone.bottom_up.res3.3.conv1.*               | backbone.bottom_up.res3.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,512,1,1)       |\n",
      "| backbone.bottom_up.res3.3.conv2.*               | backbone.bottom_up.res3.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (128,) (128,) (128,) (128,) (128,128,3,3)       |\n",
      "| backbone.bottom_up.res3.3.conv3.*               | backbone.bottom_up.res3.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,128,1,1)       |\n",
      "| backbone.bottom_up.res4.0.conv1.*               | backbone.bottom_up.res4.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,512,1,1)       |\n",
      "| backbone.bottom_up.res4.0.conv2.*               | backbone.bottom_up.res4.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| backbone.bottom_up.res4.0.conv3.*               | backbone.bottom_up.res4.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| backbone.bottom_up.res4.0.shortcut.*            | backbone.bottom_up.res4.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (1024,) (1024,) (1024,) (1024,) (1024,512,1,1)  |\n",
      "| backbone.bottom_up.res4.1.conv1.*               | backbone.bottom_up.res4.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| backbone.bottom_up.res4.1.conv2.*               | backbone.bottom_up.res4.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| backbone.bottom_up.res4.1.conv3.*               | backbone.bottom_up.res4.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| backbone.bottom_up.res4.2.conv1.*               | backbone.bottom_up.res4.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| backbone.bottom_up.res4.2.conv2.*               | backbone.bottom_up.res4.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| backbone.bottom_up.res4.2.conv3.*               | backbone.bottom_up.res4.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| backbone.bottom_up.res4.3.conv1.*               | backbone.bottom_up.res4.3.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| backbone.bottom_up.res4.3.conv2.*               | backbone.bottom_up.res4.3.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| backbone.bottom_up.res4.3.conv3.*               | backbone.bottom_up.res4.3.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| backbone.bottom_up.res4.4.conv1.*               | backbone.bottom_up.res4.4.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| backbone.bottom_up.res4.4.conv2.*               | backbone.bottom_up.res4.4.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| backbone.bottom_up.res4.4.conv3.*               | backbone.bottom_up.res4.4.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| backbone.bottom_up.res4.5.conv1.*               | backbone.bottom_up.res4.5.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,1024,1,1)      |\n",
      "| backbone.bottom_up.res4.5.conv2.*               | backbone.bottom_up.res4.5.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (256,) (256,) (256,) (256,) (256,256,3,3)       |\n",
      "| backbone.bottom_up.res4.5.conv3.*               | backbone.bottom_up.res4.5.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (1024,) (1024,) (1024,) (1024,) (1024,256,1,1)  |\n",
      "| backbone.bottom_up.res5.0.conv1.*               | backbone.bottom_up.res5.0.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,1024,1,1)      |\n",
      "| backbone.bottom_up.res5.0.conv2.*               | backbone.bottom_up.res5.0.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |\n",
      "| backbone.bottom_up.res5.0.conv3.*               | backbone.bottom_up.res5.0.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |\n",
      "| backbone.bottom_up.res5.0.shortcut.*            | backbone.bottom_up.res5.0.shortcut.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight} | (2048,) (2048,) (2048,) (2048,) (2048,1024,1,1) |\n",
      "| backbone.bottom_up.res5.1.conv1.*               | backbone.bottom_up.res5.1.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,2048,1,1)      |\n",
      "| backbone.bottom_up.res5.1.conv2.*               | backbone.bottom_up.res5.1.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |\n",
      "| backbone.bottom_up.res5.1.conv3.*               | backbone.bottom_up.res5.1.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |\n",
      "| backbone.bottom_up.res5.2.conv1.*               | backbone.bottom_up.res5.2.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,2048,1,1)      |\n",
      "| backbone.bottom_up.res5.2.conv2.*               | backbone.bottom_up.res5.2.conv2.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (512,) (512,) (512,) (512,) (512,512,3,3)       |\n",
      "| backbone.bottom_up.res5.2.conv3.*               | backbone.bottom_up.res5.2.conv3.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}    | (2048,) (2048,) (2048,) (2048,) (2048,512,1,1)  |\n",
      "| backbone.bottom_up.stem.conv1.*                 | backbone.bottom_up.stem.conv1.{norm.bias,norm.running_mean,norm.running_var,norm.weight,weight}      | (64,) (64,) (64,) (64,) (64,3,7,7)              |\n",
      "| backbone.fpn_lateral2.*                         | backbone.fpn_lateral2.{bias,weight}                                                                  | (256,) (256,256,1,1)                            |\n",
      "| backbone.fpn_lateral3.*                         | backbone.fpn_lateral3.{bias,weight}                                                                  | (256,) (256,512,1,1)                            |\n",
      "| backbone.fpn_lateral4.*                         | backbone.fpn_lateral4.{bias,weight}                                                                  | (256,) (256,1024,1,1)                           |\n",
      "| backbone.fpn_lateral5.*                         | backbone.fpn_lateral5.{bias,weight}                                                                  | (256,) (256,2048,1,1)                           |\n",
      "| backbone.fpn_output2.*                          | backbone.fpn_output2.{bias,weight}                                                                   | (256,) (256,256,3,3)                            |\n",
      "| backbone.fpn_output3.*                          | backbone.fpn_output3.{bias,weight}                                                                   | (256,) (256,256,3,3)                            |\n",
      "| backbone.fpn_output4.*                          | backbone.fpn_output4.{bias,weight}                                                                   | (256,) (256,256,3,3)                            |\n",
      "| backbone.fpn_output5.*                          | backbone.fpn_output5.{bias,weight}                                                                   | (256,) (256,256,3,3)                            |\n",
      "| proposal_generator.rpn_head.anchor_deltas.*     | proposal_generator.rpn_head.anchor_deltas.{bias,weight}                                              | (12,) (12,256,1,1)                              |\n",
      "| proposal_generator.rpn_head.conv.*              | proposal_generator.rpn_head.conv.{bias,weight}                                                       | (256,) (256,256,3,3)                            |\n",
      "| proposal_generator.rpn_head.objectness_logits.* | proposal_generator.rpn_head.objectness_logits.{bias,weight}                                          | (3,) (3,256,1,1)                                |\n",
      "| roi_heads.box_head.fc1.*                        | roi_heads.box_head.fc1.{bias,weight}                                                                 | (1024,) (1024,12544)                            |\n",
      "| roi_heads.box_head.fc2.*                        | roi_heads.box_head.fc2.{bias,weight}                                                                 | (1024,) (1024,1024)                             |\n",
      "| roi_heads.box_predictor.bbox_pred.*             | roi_heads.box_predictor.bbox_pred.{bias,weight}                                                      | (12,) (12,1024)                                 |\n",
      "| roi_heads.box_predictor.cls_score.*             | roi_heads.box_predictor.cls_score.{bias,weight}                                                      | (4,) (4,1024)                                   |\n"
     ]
    }
   ],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "# делаем конфиг\n",
    "cfg = get_cfg()\n",
    "# Дефолтный конфиг\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "\n",
    "# Отдаем веса дообученной модели\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "# Чтобы не ругался на отсутствие cuda\n",
    "cfg.MODEL.DEVICE='cpu'\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  # edge type 1, edge type 2 and node\n",
    "# Порог, по которому отбираются распознанные объекты\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "\n",
    "# Моделька-предсказатель\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boundbox Recognition Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Примеры можно поглядеть в директории из параметра dir_for_recognized_bounds\n",
    "\n",
    "dataset_dicts = get_dict_list(validation_data_dir)\n",
    "\n",
    "# Берем случайных ребят из сета\n",
    "for d in random.sample(dataset_dicts, 20):  \n",
    "    \n",
    "    # Достаем название файла с изображением графа\n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    \n",
    "    # Ищем объекты\n",
    "    outputs = predictor(im)\n",
    "    \n",
    "    # Это для картинки с распознанными баундами\n",
    "    v = Visualizer(im[:, :, ::-1], scale=0.5)\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    \n",
    "    img_name = d['file_name'].split('\\\\')[-1]\n",
    "    \n",
    "    # Рисуем, если надо, исходную картинку и картинку с распознанными обхектами\n",
    "    if draw_images_mode:\n",
    "        cv2.imwrite(os.path.join(dir_for_recognized_bounds, img_name), im)\n",
    "        cv2.imwrite(os.path.join(dir_for_recognized_bounds, img_name), out.get_image())\n",
    "    \n",
    "    # Можно вывести классы распознанных объектов и координаты их баундбоксов\n",
    "    # print(outputs[\"instances\"].pred_classes)\n",
    "    # print(outputs[\"instances\"].pred_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Общие проблемы:\n",
    "- Не распознается буква I - выбран не очень хороший шрифт для генерации (((\n",
    "- Не всегда хорошо ребра распознаются + типы\n",
    "- Стоит использовать разный порог для нод и ребер, тк ноды распознаются очень хорошо почти всегда"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes Text Detection and Graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для нахождения нод, которые связывает ребро\n",
    "def get_edge_nodes(cropped, bound_int, cnt, nodes_dict, graph_data):\n",
    "    edge_on_main = edge_on_main_diag(cropped, cnt, graph_data)  # main - слева-направо и сверху-вниз, углы нужны те, которые в баунде\n",
    "    \n",
    "    if not edge_on_main:\n",
    "        shift_x = int(abs(bound_int[2] - bound_int[0]))\n",
    "        shift_y = int(abs(bound_int[3] - bound_int[1]))\n",
    "        \n",
    "        x_1, y_1, x_2, y_2 = bound_int[0] + shift_x, bound_int[1], bound_int[2] - shift_x, bound_int[3]\n",
    "    else:\n",
    "        x_1, y_1, x_2, y_2 = bound_int[0], bound_int[1], bound_int[2], bound_int[3]\n",
    "    \n",
    "    # расстояние для x1, y1 баунда ребра до ближайшей ноды\n",
    "    min_dist_1 = 1e10\n",
    "    closest_node_1 = 0\n",
    "    # расстояние для x2, y2 баунда ребра до ближайшей ноды\n",
    "    min_dist_2 = 1e10\n",
    "    closes_node_2 = 0\n",
    "    \n",
    "    for node in nodes_dict:\n",
    "        node_center_x = nodes_dict[node]['center_x']\n",
    "        node_center_y = nodes_dict[node]['center_y']\n",
    "        \n",
    "        dist_1 = np.sqrt((node_center_x - x_1)**2 + (node_center_y - y_1)**2)\n",
    "        dist_2 = np.sqrt((node_center_x - x_2)**2 + (node_center_y - y_2)**2)\n",
    "        \n",
    "        if dist_1 < min_dist_1:\n",
    "            min_dist_1 = dist_1\n",
    "            closest_node_1 = node\n",
    "        if dist_2 < min_dist_2:\n",
    "            min_dist_2 = dist_2\n",
    "            closest_node_2 = node\n",
    "            \n",
    "    return closest_node_1, closest_node_2\n",
    "        \n",
    "    \n",
    "# Функция для проверки, на какой из диагоналей лежит ребро\n",
    "def edge_on_main_diag(cropped, cnt, graph_data):\n",
    "    \n",
    "    len_x = len(cropped[0])\n",
    "    len_y = len(cropped)\n",
    "    \n",
    "    k = len_y / len_x\n",
    "    \n",
    "    main_intensity = 0\n",
    "    sub_intensity = 0\n",
    "\n",
    "    shift = 30\n",
    "    \n",
    "    cropped_img = cropped.copy()\n",
    "    cropped_img_2 = cropped.copy()\n",
    "    for i in range(0, len_y):\n",
    "        x_diag = int(i / k)\n",
    "        \n",
    "        for j in range(x_diag - shift, x_diag + shift):\n",
    "            if 0 <= j < len_x:\n",
    "                # cnt_main += 1\n",
    "                main_intensity += sum(cropped[i][j])\n",
    "                cropped_img[i][j] = (255, 0, 0)\n",
    "\n",
    "        for j in range(len_x - x_diag - shift, len_x - x_diag + shift):\n",
    "            if 0 <= j < len_x:\n",
    "                # cnt_sub += 1\n",
    "                sub_intensity += sum(cropped[i][j])\n",
    "                cropped_img_2[i][j] = (255, 0, 0)\n",
    "                \n",
    "    if draw_images_mode:\n",
    "        img_name_wo_ext = get_img_name(graph_data)\n",
    "        \n",
    "        cv2.imwrite(os.path.join(dir_for_recognized_graphs, \n",
    "                                 img_name_wo_ext + '_cropped_edge' + str(cnt) + '_main_colored.png'), cropped_img)\n",
    "        cv2.imwrite(os.path.join(dir_for_recognized_graphs, \n",
    "                                 img_name_wo_ext + '_cropped_edge' + str(cnt) + '_sub_colored.png'), cropped_img_2)\n",
    "    \n",
    "    return main_intensity < sub_intensity\n",
    "\n",
    "\n",
    "# Функция для вытаскивания названия картинки без расширения\n",
    "def get_img_name(graph_data):\n",
    "    img_name = graph_data['file_name'].split('\\\\')[-1]\n",
    "    img_name_wo_ext = img_name.split('.')[0]\n",
    "    \n",
    "    return img_name_wo_ext\n",
    "\n",
    "\n",
    "# Функция, которая по картинке отдает метки классов распознанных обхектов, баунды и их скоры\n",
    "def get_prediction(graph_data, predictor, im):\n",
    "\n",
    "    outputs = predictor(im)\n",
    "    \n",
    "    v = Visualizer(im[:, :, ::-1], \n",
    "                   scale=0.5\n",
    "    )\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    \n",
    "    img_name_wo_ext = get_img_name(graph_data)\n",
    "    \n",
    "    if draw_images_mode:\n",
    "        cv2.imwrite(os.path.join(dir_for_recognized_graphs, img_name_wo_ext + '_clear.png'), im)\n",
    "        cv2.imwrite(os.path.join(dir_for_recognized_graphs, img_name_wo_ext + '_recognized.png'), out.get_image())\n",
    "    \n",
    "    classes = outputs[\"instances\"].pred_classes\n",
    "    bounds = outputs[\"instances\"].pred_boxes\n",
    "    scores = outputs[\"instances\"].scores\n",
    "    \n",
    "    bounds_labeled = list(zip(classes, bounds, scores))\n",
    "    \n",
    "    return bounds_labeled\n",
    "\n",
    "\n",
    "# По результатам экспериментов, 99%, что нераспознанная вторая буква это I\n",
    "def fix_text_label(text):\n",
    "    # убираем перевод строки\n",
    "    text = text.split('\\n')[0]\n",
    "            \n",
    "    # допустим он потерял букву I - все остальные распознаются ок (ну и плюс мы не потеряем ноду)\n",
    "    if len(text) == 1:\n",
    "        text = text + 'I'\n",
    "        \n",
    "    return text\n",
    "\n",
    "\n",
    "# Функция, которая нахоидт все ноды на картинке, распознает их текстовые метки и отдает словарик с нужными данными\n",
    "def get_nodes_dict(graph_data, bounds_labeled, im, cnt=0):\n",
    "    \n",
    "    nodes_dict = dict()\n",
    "    \n",
    "    if draw_images_mode: cnt = 0  # счетчик для названий, под окторыми будем вырисовывать ноды/ребра\n",
    "        \n",
    "    img_name_wo_ext = get_img_name(graph_data)\n",
    "    \n",
    "    if write_recognized_text:\n",
    "        # Сюда прилетят все распознанные текстовые метки\n",
    "        file = open(os.path.join(dir_for_recognized_graphs, img_name_wo_ext + '_recognized_text.txt'), \"a\")\n",
    "\n",
    "    # идем по всем нодам и распознаем их метки\n",
    "    for bound in bounds_labeled:\n",
    "        \n",
    "        # если не нода или если уверенность ниже 95%\n",
    "        if bound[0] != 0 or bound[2] < 0.95:\n",
    "            continue\n",
    "            \n",
    "        # Исходно все значения float, а мы работаем с пикселями\n",
    "        bound_int = [int(val) for val in bound[1]]  # x1, y1, x2, y2\n",
    "        \n",
    "        im2 = im.copy()\n",
    "        \n",
    "        # Сужаем границы баундбокса тк иначе ничего не распознать\n",
    "        shift_x = int(abs(bound_int[2] - bound_int[0])*0.25)\n",
    "        shift_y = int(abs(bound_int[3] - bound_int[1])*0.25)\n",
    "        \n",
    "        cropped = im2[bound_int[1] + shift_y:bound_int[3] - shift_y, \n",
    "                      bound_int[0] + shift_x:bound_int[2] - shift_x]\n",
    "        \n",
    "        if draw_images_mode:\n",
    "            # Рисуем кропленную ноду, над которой будем стараться\n",
    "            cv2.imwrite(os.path.join(dir_for_recognized_graphs, \n",
    "                                     img_name_wo_ext + '_cropped_node' + str(cnt) + '.png'), \n",
    "                        cropped) \n",
    "            cnt += 1\n",
    "        \n",
    "        # Распознавание текста\n",
    "        text = pytesseract.image_to_string(cropped, config=custom_config)\n",
    "        \n",
    "        # убираем перевод строки и всякое такое\n",
    "        text = fix_text_label(text)\n",
    "        \n",
    "        # текст не распознался - а угадывать метку ноды наверное не прикол, но вообще сильновероятно, что нода там была...\n",
    "        if text:\n",
    "            \n",
    "            if write_recognized_text:\n",
    "                file.write(text + '\\n')\n",
    "\n",
    "            nodes_dict[text] = {'bbox': bound_int, \n",
    "                                'score': bound[2],\n",
    "                                'center_y': int((bound_int[3] + bound_int[1])*0.5), \n",
    "                                'center_x': int((bound_int[2] + bound_int[0])*0.5)}\n",
    "    \n",
    "    if write_recognized_text:\n",
    "        file.close()\n",
    "    \n",
    "    return nodes_dict\n",
    "\n",
    "\n",
    "# Если нода нашлась, но она не попала в связный граф, то я все равно хочу видеть ее в итоговом списке смежности\n",
    "def add_not_connected_nodes(nodes_dict, adj_list):\n",
    "    for node in nodes_dict:\n",
    "        if node not in adj_list:\n",
    "            adj_list[node] = []\n",
    "    \n",
    "    return adj_list\n",
    "\n",
    "\n",
    "# Функция клепает список смежности по распознанным данным\n",
    "def create_adj_list(graph_data, bounds_labeled, im, nodes_dict, cnt=0):\n",
    "    \n",
    "    adj_list = defaultdict(dict)\n",
    "    # score_dict = defaultdict(dict)\n",
    "    img_name_wo_ext = get_img_name(graph_data)\n",
    "    im2 = im.copy()\n",
    "    \n",
    "    for bound in bounds_labeled:\n",
    "        \n",
    "        if bound[0] == 0:\n",
    "            continue\n",
    "        \n",
    "        bound_int = [int(val) for val in bound[1]]  # x1, y1, x2, y2\n",
    "        \n",
    "        bound_score = bound[2]\n",
    "        bound_class = int(bound[0])\n",
    "        bound_type = bound_class - 1  # Типы ребер 0 и 1 превратились в 1 и 2 классы обхектов для нейронки, \n",
    "                                      # теперь веду их обратно из класса в тип\n",
    "        \n",
    "        cropped = im2[bound_int[1]:bound_int[3], bound_int[0]:bound_int[2]]\n",
    "        \n",
    "        if draw_images_mode:\n",
    "            # записываем картинку с кропленным ребром, над которым сейчас будем стараться\n",
    "            cv2.imwrite(os.path.join(dir_for_recognized_graphs, \n",
    "                                     img_name_wo_ext + '_cropped_edge' + str(cnt) + '.png'), cropped) \n",
    "        \n",
    "        # Ищем ноды, которые связывает наше ребро\n",
    "        node_1, node_2 = get_edge_nodes(cropped, bound_int, cnt, nodes_dict, graph_data)\n",
    "        \n",
    "        # Ребро уже в базе\n",
    "        if node_1 in adj_list and node_2 in adj_list[node_1]:\n",
    "            # Хотела сначала чекать score ребра, но на практике выяснилась простая идея:\n",
    "            # Если ребро распозналось более 1 раза и хотя бы одно из них распозналось как двойное, \n",
    "            # то это скорее всего двойное ребро, как ни крути\n",
    "            if adj_list[node_1][node_2]['type'] == 0 or bound_type == 1:  # Если было двойное ребро или новое ребро одиночное\n",
    "                continue\n",
    "            \n",
    "        # И записываем их в список смежности\n",
    "        adj_list[node_1][node_2] = {'weigth': '1', 'type': bound_type}\n",
    "        adj_list[node_2][node_1] = {'weigth': '1', 'type': bound_type}\n",
    "        \n",
    "        # Отдельно запоминаем score тк структура выходного словаря утверждена, а значение хранить хочется\n",
    "        # score_dict[node_1][node_2] = {'score': bound_score}\n",
    "        # score_dict[node_2][node_1] = {'score': bound_score}\n",
    "        \n",
    "        if draw_images_mode:\n",
    "            cnt += 1\n",
    "            \n",
    "    return add_not_connected_nodes(nodes_dict, adj_list)\n",
    "\n",
    "\n",
    "# Распознавалка main\n",
    "def recognize_graph(graph_data, predictor):\n",
    "    \n",
    "    im = cv2.imread(graph_data[\"file_name\"])\n",
    "    \n",
    "    bounds_labeled = get_prediction(graph_data, predictor, im)\n",
    "    nodes_dict = get_nodes_dict(graph_data, bounds_labeled, im)\n",
    "    adj_list = create_adj_list(graph_data, bounds_labeled, im, nodes_dict)\n",
    "    \n",
    "    return adj_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running with random graph from val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Берем все данные из валидационного сета - вообще тестового, но они у себя зовут его валидационным так и оставим\n",
    "dataset_dicts = get_dict_list(validation_data_dir)\n",
    "\n",
    "for graph_data in random.sample(dataset_dicts, 1): \n",
    "    \n",
    "    adj_list = recognize_graph(graph_data, predictor)\n",
    "    img_name_wo_ext = get_img_name(graph_data)\n",
    "    \n",
    "    with open(os.path.join(dir_for_recognized_graphs, img_name_wo_ext + '_recognized_graph.pickle'), 'wb') as handle:\n",
    "        pickle.dump(adj_list, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running with image name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_image = os.path.join('D:\\Projects\\Anaconda\\Signals_ml_22\\graph-recognizer\\graph_img_val', 'graph_89.png')\n",
    "\n",
    "graph_data = {'file_name': path_to_image}\n",
    "adj_list = recognize_graph(graph_data, predictor)\n",
    "img_name_wo_ext = get_img_name(graph_data)\n",
    "    \n",
    "with open(os.path.join(dir_for_recognized_graphs, img_name_wo_ext + '_recognized_graph.pickle'), 'wb') as handle:\n",
    "    pickle.dump(adj_list, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple recognition cheker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj_list(dir_name: str, graph_data, recogn = False):\n",
    "    img_name_wo_ext = get_img_name(graph_data)\n",
    "    \n",
    "    if recogn:\n",
    "        img_name_wo_ext += '_recognized_graph'\n",
    "    with open(os.path.join(dir_name, img_name_wo_ext + '.pickle'), 'rb') as handle:\n",
    "        adj_list = pickle.load(handle)\n",
    "        \n",
    "    return adj_list\n",
    "\n",
    "\n",
    "def check_diff(graph_data, pred_adj_list=None):\n",
    "    src_adj_list = get_adj_list(validation_data_dir, graph_data)\n",
    "    \n",
    "    if not pred_adj_list:\n",
    "        pred_adj_list = get_adj_list(dir_for_recognized_graphs, graph_data, recogn = True)\n",
    "    \n",
    "    matched_edges = 0\n",
    "    matched_types = 0\n",
    "    matched_nodes = 0\n",
    "    \n",
    "    wrong_type = 0\n",
    "    lost_edge = 0\n",
    "    lost_node = 0\n",
    "    \n",
    "    src_edges_num = 0\n",
    "    src_nodes_num = 0\n",
    "    \n",
    "    for node in src_adj_list:\n",
    "        src_nodes_num += 1\n",
    "        if node in pred_adj_list: \n",
    "            matched_nodes += 1\n",
    "            \n",
    "            for connected_node in src_adj_list[node]:\n",
    "                \n",
    "                src_edges_num += 1\n",
    "\n",
    "                src_type = src_adj_list[node][connected_node]['type']\n",
    "\n",
    "                if connected_node in pred_adj_list[node]:\n",
    "                    \n",
    "                    pred_type = pred_adj_list[node][connected_node]['type']\n",
    "\n",
    "                    matched_edges += 1\n",
    "                    if pred_type == src_type:\n",
    "                        matched_types += 1\n",
    "                    else:\n",
    "                        wrong_type += 1\n",
    "                else:\n",
    "                    lost_edge += 1\n",
    "        else:\n",
    "            lost_node += 1\n",
    "    \n",
    "    res = {\n",
    "        # количество нод, которые были распознаны среди исходных\n",
    "        'matched_nodes': matched_nodes,\n",
    "        # всего нод было в графе\n",
    "        'all_nodes_num': src_nodes_num,\n",
    "        # частота верно распознанных нод\n",
    "        'recognized_nodes_freq': matched_nodes / src_nodes_num,\n",
    "        # кол-во ненайденных нод среди исходных\n",
    "        'lost_nodes': lost_node // 2,\n",
    "        # частота ненахождения ноды\n",
    "        'lost_nodes_freq': lost_node / src_nodes_num,\n",
    "        \n",
    "        # кол-во найденныз ребер среди исходных\n",
    "        'matched_edges': matched_edges // 2,\n",
    "        # всего было ребер исходно\n",
    "        'all_edges_num': src_edges_num // 2,\n",
    "        # частота верного распознавания ребер\n",
    "        'recognized_edges_freq': matched_edges / src_edges_num,\n",
    "        \n",
    "        # кол-во потерянных ребер\n",
    "        'lost_edges': lost_edge // 2,\n",
    "        # частота потери ребер\n",
    "        'lost_edges_freq': lost_edge / src_edges_num,\n",
    "        \n",
    "        # кол-во верно распознанных типов ребер среди верно распознанных ребер\n",
    "        'matched_edge_types': matched_types // 2,\n",
    "        # частота верно распознанных типов ребер среди верно расп. ребер\n",
    "        'matched_edge_types_freq': matched_types / matched_edges,\n",
    "        \n",
    "        # кол-во неверно распознанных типов ребер среди верно распознанных ребер\n",
    "        'wrong_edge_types': wrong_type // 2,\n",
    "        # частота неверно распознанных типов ребер среди верно расп. ребер\n",
    "        'wrong_edge_types_freq': wrong_type / matched_edges,\n",
    "    }\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checker res example for graph_105 \n",
    "\n",
    "Результаты распознавания находятся в директории репо graph_recognition_res/graph_105/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matched_nodes': 9,\n",
       " 'all_nodes_num': 9,\n",
       " 'recognized_nodes_freq': 1.0,\n",
       " 'lost_nodes': 0,\n",
       " 'lost_nodes_freq': 0.0,\n",
       " 'matched_edges': 8,\n",
       " 'all_edges_num': 8,\n",
       " 'recognized_edges_freq': 1.0,\n",
       " 'lost_edges': 0,\n",
       " 'lost_edges_freq': 0.0,\n",
       " 'matched_edge_types': 8,\n",
       " 'matched_edge_types_freq': 1.0,\n",
       " 'wrong_edge_types': 0,\n",
       " 'wrong_edge_types_freq': 0.0}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_image = os.path.join('D:\\Projects\\Anaconda\\Signals_ml_22\\graph-recognizer\\graph_img_val', 'graph_105.png')\n",
    "\n",
    "graph_data = {'file_name': path_to_image}\n",
    "adj_list = recognize_graph(graph_data, predictor)\n",
    "img_name_wo_ext = get_img_name(graph_data)\n",
    "    \n",
    "with open(os.path.join(dir_for_recognized_graphs, img_name_wo_ext + '_recognized_graph.pickle'), 'wb') as handle:\n",
    "    pickle.dump(adj_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# Первый аргумент - словарик с названием картинки, он будет искать в той же директории .pickle со списком смежности исходного графа\n",
    "# Второй аргумент - либо пустой (тогда он ищет в своей директории с результатами записанный .pickle), либо расп. список смежности\n",
    "check_diff(graph_data, adj_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b2647bf07d49429f40b02945220dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=108.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Берем все данные из валидационного сета - вообще тестового, но они у себя зовут его валидационным так и оставим\n",
    "dataset_dicts = get_dict_list(validation_data_dir)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for graph_data in tqdm(dataset_dicts): \n",
    "    \n",
    "    adj_list = recognize_graph(graph_data, predictor)\n",
    "    img_name_wo_ext = get_img_name(graph_data)\n",
    "    \n",
    "    metrics = check_diff(graph_data, adj_list)\n",
    "    df = df.append(metrics, ignore_index=True)\n",
    "    \n",
    "df.to_csv('metrics.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_edges_num</th>\n",
       "      <th>all_nodes_num</th>\n",
       "      <th>lost_edges</th>\n",
       "      <th>lost_edges_freq</th>\n",
       "      <th>lost_nodes</th>\n",
       "      <th>lost_nodes_freq</th>\n",
       "      <th>matched_edge_types</th>\n",
       "      <th>matched_edge_types_freq</th>\n",
       "      <th>matched_edges</th>\n",
       "      <th>matched_nodes</th>\n",
       "      <th>recognized_edges_freq</th>\n",
       "      <th>recognized_nodes_freq</th>\n",
       "      <th>wrong_edge_types</th>\n",
       "      <th>wrong_edge_types_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   all_edges_num  all_nodes_num  lost_edges  lost_edges_freq  lost_nodes  \\\n",
       "0            1.0            2.0         0.0         0.000000         0.0   \n",
       "1           10.0           11.0         2.0         0.200000         0.0   \n",
       "2            7.0            8.0         1.0         0.142857         0.0   \n",
       "3            7.0            8.0         0.0         0.000000         0.0   \n",
       "4            6.0            7.0         1.0         0.166667         0.0   \n",
       "\n",
       "   lost_nodes_freq  matched_edge_types  matched_edge_types_freq  \\\n",
       "0              0.0                 1.0                      1.0   \n",
       "1              0.0                 8.0                      1.0   \n",
       "2              0.0                 6.0                      1.0   \n",
       "3              0.0                 7.0                      1.0   \n",
       "4              0.0                 5.0                      1.0   \n",
       "\n",
       "   matched_edges  matched_nodes  recognized_edges_freq  recognized_nodes_freq  \\\n",
       "0            1.0            2.0               1.000000                    1.0   \n",
       "1            8.0           11.0               0.800000                    1.0   \n",
       "2            6.0            8.0               0.857143                    1.0   \n",
       "3            7.0            8.0               1.000000                    1.0   \n",
       "4            5.0            7.0               0.833333                    1.0   \n",
       "\n",
       "   wrong_edge_types  wrong_edge_types_freq  \n",
       "0               0.0                    0.0  \n",
       "1               0.0                    0.0  \n",
       "2               0.0                    0.0  \n",
       "3               0.0                    0.0  \n",
       "4               0.0                    0.0  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = df[['recognized_nodes_freq', 'recognized_edges_freq', \n",
    "    'lost_nodes_freq', 'lost_edges_freq', \n",
    "    'matched_edge_types_freq', 'wrong_edge_types_freq'\n",
    "   ]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя частота верно распознанных нод:  0.9956349206349208\n",
      "Средняя частота потери нод:  0.004365079365079365\n",
      "Средняя частота верного распознавания ребер:  0.9435060567005008\n",
      "Средняя частота потери ребер:  0.05649394329949885\n",
      "Средняя частота верно распознанных типов ребер среди верно расп. ребер:  0.9472001763668426\n",
      "Средняя частота неверно распознанных типов ребер среди верно расп. ребер:  0.05279982363315696\n"
     ]
    }
   ],
   "source": [
    "print('Средняя частота верно распознанных нод: ', stat['recognized_nodes_freq'])\n",
    "\n",
    "print('Средняя частота потери нод: ', stat['lost_nodes_freq'])\n",
    "\n",
    "print('Средняя частота верного распознавания ребер: ', stat['recognized_edges_freq'])\n",
    "\n",
    "print('Средняя частота потери ребер: ', stat['lost_edges_freq'])\n",
    "\n",
    "print('Средняя частота верно распознанных типов ребер среди верно расп. ребер: ', stat['matched_edge_types_freq'])\n",
    "        \n",
    "print('Средняя частота неверно распознанных типов ребер среди верно расп. ребер: ', stat['wrong_edge_types_freq'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
